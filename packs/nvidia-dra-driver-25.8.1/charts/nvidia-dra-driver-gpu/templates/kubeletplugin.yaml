# Copyright 2023 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{- if or .Values.resources.computeDomains.enabled .Values.resources.gpus.enabled }}
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: {{ include "nvidia-dra-driver-gpu.name" . }}-kubelet-plugin
  namespace: {{ include "nvidia-dra-driver-gpu.namespace" . }}
  labels:
    {{- include "nvidia-dra-driver-gpu.labels" . | nindent 4 }}
spec:
  selector:
    matchLabels:
      {{- include "nvidia-dra-driver-gpu.selectorLabels" (dict "context" . "componentName" "kubelet-plugin") | nindent 6 }}
  {{- with .Values.kubeletPlugin.updateStrategy }}
  updateStrategy:
    {{- toYaml . | nindent 4 }}
  {{- end }}
  template:
    metadata:
      {{- with .Values.kubeletPlugin.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "nvidia-dra-driver-gpu.templateLabels" . | nindent 8 }}
        {{- include "nvidia-dra-driver-gpu.selectorLabels" (dict "context" . "componentName" "kubelet-plugin") | nindent 8 }}
    spec:
      {{- if .Values.kubeletPlugin.priorityClassName }}
      priorityClassName: {{ .Values.kubeletPlugin.priorityClassName }}
      {{- end }}
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "nvidia-dra-driver-gpu.serviceAccountName" . }}-kubeletplugin
      securityContext:
        {{- toYaml .Values.kubeletPlugin.podSecurityContext | nindent 8 }}
      initContainers:
      - name: init-container
        image: {{ include "nvidia-dra-driver-gpu.fullimage" . }}
        securityContext:
          privileged: true
        command: [bash, /usr/bin/kubelet-plugin-prestart.sh]
        env:
        - name: NVIDIA_DRIVER_ROOT
          value: "{{ .Values.nvidiaDriverRoot }}"
        # Use runc: explicit "void"; otherwise we inherit "all".
        - name: NVIDIA_VISIBLE_DEVICES
          value: void
        - name: KUBELET_REGISTRAR_DIRECTORY_PATH
          value: {{ .Values.kubeletPlugin.kubeletRegistrarDirectoryPath | quote }}
        - name: KUBELET_PLUGINS_DIRECTORY_PATH
          value: {{ .Values.kubeletPlugin.kubeletPluginsDirectoryPath | quote }}
        volumeMounts:
        - name: driver-root-parent
          mountPath: /driver-root-parent
          {{- if eq "/" .Values.nvidiaDriverRoot }}
          readOnly: true
          {{- else }}
          # In case of the operator-provided driver, another container mounts
          # the driver onto the host using `mountPropagation: Bidirectional`
          # (out-of-band of the lifecycle of _this_ pod here). For us to see
          # that mount, `mountPropagation: HostToContainer` is required (docs:
          # "if any Pod with Bidirectional mount propagation to the same volume
          # mounts anything there, the container with HostToContainer mount
          # propagation will see it.").
          mountPropagation: HostToContainer
          {{- end }}
      containers:
      {{- if .Values.resources.computeDomains.enabled }}
      - name: compute-domains
        securityContext:
          {{- toYaml .Values.kubeletPlugin.containers.computeDomains.securityContext | nindent 10 }}
        image: {{ include "nvidia-dra-driver-gpu.fullimage" . }}
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        command: ["bash", "-c"]
        args:
        - |-
          # Conditionally mask the params file to prevent this container from
          # recreating any missing GPU device nodes. This is necessary, for
          # example, when running under nvkind to limit the set GPUs governed
          # by the plugin even though it has cgroup access to all of them.
          if [ "${MASK_NVIDIA_DRIVER_PARAMS}" = "true" ]; then
            cp /proc/driver/nvidia/params root/gpu-params
            sed -i 's/^ModifyDeviceFiles: 1$/ModifyDeviceFiles: 0/' root/gpu-params
            mount --bind root/gpu-params /proc/driver/nvidia/params
          fi
          compute-domain-kubelet-plugin -v $(LOG_VERBOSITY)
        resources:
          {{- toYaml .Values.kubeletPlugin.containers.computeDomains.resources | nindent 10 }}
        {{/*
          A literal "0" will allocate a random port. Don't configure the probe
          with the same literal "0" since that won't match where the service is
          actually running.
        */}}
        {{- if (gt (int .Values.kubeletPlugin.containers.computeDomains.healthcheckPort) 0) }}
        startupProbe:
          grpc:
            port: {{ .Values.kubeletPlugin.containers.computeDomains.healthcheckPort }}
            service: liveness
          failureThreshold: 60
          periodSeconds: 10
          timeoutSeconds: 10
        livenessProbe:
          grpc:
            port: {{ .Values.kubeletPlugin.containers.computeDomains.healthcheckPort }}
            service: liveness
          failureThreshold: 3
          periodSeconds: 10
          timeoutSeconds: 10
        {{- end }}
        env:
        # LOG_VERBOSITY is the source of truth for this program's klog
        # configuration. Currently injected via CLI argument (see above) because
        # klog's verbosity for now cannot be sanely set from an environment
        # variable.
        - name: LOG_VERBOSITY
          value: "{{ .Values.logVerbosity }}"
        - name: MASK_NVIDIA_DRIVER_PARAMS
          value: "{{ .Values.maskNvidiaDriverParams }}"
        - name: NVIDIA_DRIVER_ROOT
          value: "{{ .Values.nvidiaDriverRoot }}"
        - name: NVIDIA_VISIBLE_DEVICES
          value: void
        - name: CDI_ROOT
          value: /var/run/cdi
        - name: NVIDIA_MIG_CONFIG_DEVICES
          value: all
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        {{- if .Values.nvidiaCDIHookPath }}
        - name: NVIDIA_CDI_HOOK_PATH
          value: "{{ .Values.nvidiaCDIHookPath }}"
        {{- end }}
        {{- if .Values.featureGates }}
        - name: FEATURE_GATES
          value: "{{ range $key, $value := .Values.featureGates }}{{ $key }}={{ $value }},{{ end }}"
        {{- end }}
        - name: KUBELET_REGISTRAR_DIRECTORY_PATH
          value: {{ .Values.kubeletPlugin.kubeletRegistrarDirectoryPath | quote }}
        - name: KUBELET_PLUGINS_DIRECTORY_PATH
          value: {{ .Values.kubeletPlugin.kubeletPluginsDirectoryPath | quote }}
        {{- if .Values.kubeletPlugin.containers.computeDomains.healthcheckPort }}
        - name: HEALTHCHECK_PORT
          value: {{ .Values.kubeletPlugin.containers.computeDomains.healthcheckPort | quote }}
        {{- end }}
        {{- with .Values.kubeletPlugin.containers.computeDomains.env }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
        volumeMounts:
        - name: plugins-registry
          mountPath: {{ .Values.kubeletPlugin.kubeletRegistrarDirectoryPath | quote }}
        - name: plugins
          mountPath: {{ .Values.kubeletPlugin.kubeletPluginsDirectoryPath | quote }}
          mountPropagation: Bidirectional
        - name: cdi
          mountPath: /var/run/cdi
        - name: driver-root
          mountPath: /driver-root
          readOnly: true
          mountPropagation: HostToContainer
      {{- end }}
      {{- if .Values.resources.gpus.enabled }}
      - name: gpus
        securityContext:
          {{- toYaml .Values.kubeletPlugin.containers.gpus.securityContext | nindent 10 }}
        image: {{ include "nvidia-dra-driver-gpu.fullimage" . }}
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        command: ["bash", "-c"]
        args:
        - |-
          # Conditionally mask the params file to prevent this container from
          # recreating any missing GPU device nodes. This is necessary, for
          # example, when running under nvkind to limit the set GPUs governed
          # by the plugin even though it has cgroup access to all of them.
          if [ "${MASK_NVIDIA_DRIVER_PARAMS}" = "true" ]; then
            cp /proc/driver/nvidia/params root/gpu-params
            sed -i 's/^ModifyDeviceFiles: 1$/ModifyDeviceFiles: 0/' root/gpu-params
            mount --bind root/gpu-params /proc/driver/nvidia/params
          fi
          gpu-kubelet-plugin -v $(LOG_VERBOSITY)
        resources:
          {{- toYaml .Values.kubeletPlugin.containers.gpus.resources | nindent 10 }}
        {{/*
          A literal "0" will allocate a random port. Don't configure the probe
          with the same literal "0" since that won't match where the service is
          actually running.
        */}}
        {{- if (gt (int .Values.kubeletPlugin.containers.gpus.healthcheckPort) 0) }}
        startupProbe:
          grpc:
            port: {{ .Values.kubeletPlugin.containers.gpus.healthcheckPort }}
            service: liveness
          failureThreshold: 60
          periodSeconds: 10
          timeoutSeconds: 10
        livenessProbe:
          grpc:
            port: {{ .Values.kubeletPlugin.containers.gpus.healthcheckPort }}
            service: liveness
          failureThreshold: 3
          periodSeconds: 10
          timeoutSeconds: 10
        {{- end }}
        env:
        # LOG_VERBOSITY is the source of truth for this program's klog
        # configuration. Currently injected via CLI argument (see above) because
        # klog's verbosity for now cannot be sanely set from an environment
        # variable.
        - name: LOG_VERBOSITY
          value: "{{ .Values.logVerbosity }}"
        - name: MASK_NVIDIA_DRIVER_PARAMS
          value: "{{ .Values.maskNvidiaDriverParams }}"
        - name: NVIDIA_DRIVER_ROOT
          value: "{{ .Values.nvidiaDriverRoot }}"
        - name: NVIDIA_VISIBLE_DEVICES
          value: void
        - name: CDI_ROOT
          value: /var/run/cdi
        - name: NVIDIA_MIG_CONFIG_DEVICES
          value: all
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: IMAGE_NAME
          value: {{ include "nvidia-dra-driver-gpu.fullimage" . }}
        {{- if .Values.nvidiaCDIHookPath }}
        - name: NVIDIA_CDI_HOOK_PATH
          value: "{{ .Values.nvidiaCDIHookPath }}"
        {{- end }}
        {{- if .Values.featureGates }}
        - name: FEATURE_GATES
          value: "{{ range $key, $value := .Values.featureGates }}{{ $key }}={{ $value }},{{ end }}"
        {{- end }}
        - name: KUBELET_REGISTRAR_DIRECTORY_PATH
          value: {{ .Values.kubeletPlugin.kubeletRegistrarDirectoryPath | quote }}
        - name: KUBELET_PLUGINS_DIRECTORY_PATH
          value: {{ .Values.kubeletPlugin.kubeletPluginsDirectoryPath | quote }}
        {{- if .Values.kubeletPlugin.containers.gpus.healthcheckPort }}
        - name: HEALTHCHECK_PORT
          value: {{ .Values.kubeletPlugin.containers.gpus.healthcheckPort | quote }}
        {{- end }}
        {{- with .Values.kubeletPlugin.containers.gpus.env }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
        volumeMounts:
        - name: plugins-registry
          mountPath: {{ .Values.kubeletPlugin.kubeletRegistrarDirectoryPath | quote }}
        - name: plugins
          mountPath: {{ .Values.kubeletPlugin.kubeletPluginsDirectoryPath | quote }}
          mountPropagation: Bidirectional
        - name: cdi
          mountPath: /var/run/cdi
        - name: driver-root
          mountPath: /driver-root
          readOnly: true
          mountPropagation: HostToContainer
      {{- end }}
      volumes:
      - name: plugins-registry
        hostPath:
          path: {{ .Values.kubeletPlugin.kubeletRegistrarDirectoryPath | quote }}
      - name: plugins
        hostPath:
          path: {{ .Values.kubeletPlugin.kubeletPluginsDirectoryPath | quote }}
      - name: cdi
        hostPath:
          path: /var/run/cdi
      - name: driver-root-parent
        hostPath:
          # If nvidiaDriverRoot == "/" then its parent is itself. Otherwise, get
          # its parent by removing any trailing slashes as well as the last path
          # element with sprig template function `dir`. Examples: /a/b/ -> /a,
          # /a/b/c -> /a/b.
          {{- if eq "/" .Values.nvidiaDriverRoot }}
          path: "/"
          {{- else }}
          path: {{ dir (trimSuffix "/" .Values.nvidiaDriverRoot) }}
          {{- end }}
          type: DirectoryOrCreate
      - name: driver-root
        hostPath:
          path: {{ .Values.nvidiaDriverRoot }}
          type: DirectoryOrCreate
      {{- with .Values.kubeletPlugin.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.kubeletPlugin.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.kubeletPlugin.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
{{- end }}
